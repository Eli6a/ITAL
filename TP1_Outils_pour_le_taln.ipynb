{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90602f82-b859-456c-ba13-d4287adfaaa4",
   "metadata": {},
   "source": [
    "# TP1: Outils pour le TALN\n",
    "\n",
    "* Cours: Traitement du langage naturel\n",
    "* Auteur: Ygor GALLINA\n",
    "* Date: Janvier 2024\n",
    "\n",
    "## Préambule\n",
    "\n",
    "Le but de ce TP est d’appréhender et de prendre en main les outils de traitement automatique de la langue existant pour traiter des données textuelles.\n",
    "\n",
    "Notre cas d’usage est d'analyser les discours de nouvelle année des présidents de la république française. Ces discours sont disponible sur la plateforme [vie-publique.fr](https://www.vie-publique.fr/qui-sommes-nous) qui recense (entre autre) les discours provenant du gouvernement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1894458-e80a-4303-ae69-60e6cb08b30d",
   "metadata": {},
   "source": [
    "### Google Collab\n",
    "\n",
    "Si vous executez ce notebook avec Google Collab exécutez la commande suivante pour connaître le pays dans lequel se trouve le serveur qui exécute votre code. Naviguez ensuite vers [electricitymaps.com](https://app.electricitymaps.com/map) pour connaitre le mix electrique de ce pays.\n",
    "\n",
    "> Le **mix énergétique**, ou bouquet énergétique, est la **répartition** des différentes **sources d'énergies** primaires **consommées** dans une zone géographique donnée. ... Le **mix électrique**, avec lequel il ne doit pas être confondu, ne prend en compte que **les sources d'énergie contribuant à la production d'électricité** ; or l'électricité ne représente que 18,5 % de la consommation finale d'énergie au niveau mondial.  Source: [Wikipédia](https://fr.wikipedia.org/wiki/Mix_%C3%A9nerg%C3%A9tique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf8ef8-ce52-462f-b890-05236f9f1ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl ipinfo.io\n",
    "# Ou bien\n",
    "# !curl https://api.country.is/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba5203-7624-437f-9931-3fc3516d4cc5",
   "metadata": {},
   "source": [
    "## Exercice 1: Pré-traitements\n",
    "\n",
    "L'objectif de cet exercice est de se familiariser avec les différents pré-traitement utilisés dans le TALN. Pour cela n'hésitez pas a consulter la documentation de chacune des librairies pour comprendre comment elles fonctionnent et à quoi correspondent les arguments de leurs fonctions.\n",
    "\n",
    "L'objectif de ce TP est de chercher dans un corpus de document les phrases qui traitent de montagnes. Pour cela différentes techniques de traitement automatique des langues (TAL, en: NLP) devront être utilisées: la segmentation en phrase, en tokens, la normalisation, l'étiquetage morphosyntaxique.\n",
    "\n",
    "## Prise en main du corpus\n",
    "\n",
    "Un projet de TAL commence toujours par le choix d'un corpus et son exploration. Nous utiliserons ici les .....\n",
    "\n",
    "### Bash\n",
    "\n",
    "Les outils en ligne de commande permettent des traitement simples et rapide à  effectuer.\n",
    "\n",
    "La commande `sed` permet de remplacer un motif par une chaîne de caractère, elle fonctionne ligne par ligne, elle s'utilise de la façon suivante:\n",
    "`sed 's/MOTIF/REMPLACEMENT/g'`, le `s` signifie substitution et le `g` global ce sont des drapeau (flags). Les caractères `/` séparent les différentes parties de la commande et peuvent être n'importe quel autre caractère (`sed 's°chats?°chat°g'` est une commande valide).\n",
    "\n",
    "La commande `grep` permet de filtrer les lignes d'un fichier suivant un motif.\n",
    "\n",
    "N'hésitez pas à consulter le `man`uel des commandes pour plus d'information (pour rechercher dans une page de manuel: taper `/`, écrire un mot, valider avec entrée, taper `n` pour la prochaine occurence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60446e-88d7-4be9-975f-8e9a3d083c44",
   "metadata": {},
   "source": [
    "1. Quel est le nombre de lignes et de mots dans l'ensemble des documents ? (commande `wc`)\n",
    "    - 885 lignes et 28395 mots (`cat *.txt | wc -l -w`)\n",
    "2. Que fait cette commande ? `cat *.txt| sed -E 's/([[:alnum:]])([\\?\\!.])/\\1 \\2/g' | sed -E 's/ +/\\n/g'`.\n",
    "    - cat *.txt : on récupère les fichiers \n",
    "3. A l'aide des commandes `uniq` et `sort` afficher les 10 tokens les plus fréquents.\n",
    "    - Votre réponse\n",
    "4. Combien de types (tokens unique) comporte le texte ?\n",
    "    - Votre réponse\n",
    "5. En regardant les tokens, identifiez en 2 qui pourraient être mieux segmentés.\n",
    "    - Votre réponse\n",
    "6. A l'aide de la commande `grep` selectionnez les types de tokens que vous avez identifié à l'étape précedente. Donnez 3 exemples de chaque.\n",
    "    - Vos réponses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8adad-98dd-409d-bc80-96790806cba4",
   "metadata": {},
   "source": [
    "### Python et NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465af96e-749c-426a-884f-7f3318838cdd",
   "metadata": {},
   "source": [
    "1. Téléchargez les données si ce n'est pas déjà fait et ouvrez un notebook à l'aide de la commande `jupyter notebook`.\n",
    "2. Chargez les données à l'aide du code ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60235157-626c-4bbf-ae5a-abee5e961b18",
   "metadata": {},
   "source": [
    "3. Utilisez la bibliothèque nltk et la fonction `nltk.word_tokenize` pour tokeniser le corpus.\n",
    "   * Est-ce que les tokens qui étaient mal segmentés à la question 6. le sont toujours ?\n",
    "    - Vous pouvez chercher dans une liste avec une compréhension de liste comme `[t for t in MONVOCAB.items() if 'chat' in t]`)\n",
    "    - Ou encore en écrivant tout les mots dans un fichier, que vous pourrez parcourir à l'aide d'un éditeur de texte.\n",
    "- Votre réponse\n",
    "\n",
    "\n",
    "4. Ecrivez ensuite une fonction `pretreat` qui prend en entrée un document tokénisé et renvoie pour chaque mot son étiquette morpho-syntaxique (ou POS tag) ainsi que sa version racinisée (ou stem).\n",
    "   * Un document sera de la forme `[('TOKEN', 'POSTAG', 'STEM'), ('TOKEN', 'POSTAG', 'STEM'), ...]`\n",
    "   * Pour les étiquettes morpho-syntaxiques vous pourrez utiliser la fonction `nltk.pos_tag` (les étiquettes résultant de cette fonction proviennent de l'universal dependencies et sont explicités sur [cette page](https://universaldependencies.org/u/pos/index.html), ce jeu d'étiquette est commun à l'ensemble des langues ! [Cette page](https://universaldependencies.org/) liste pour chaque langue ses spécificités.)\n",
    "   * Pour la racinisation, l'algorithme de Porter adapté au français est disponible dans le `nltk.stem.SnowBallStemmer`\n",
    "   * Etudiez quelques documents pour vérifier la qualité des étiquettes morphosyntaxiques, et la forme racinisée des mots.\n",
    "   * Les étiquettes morphosyntaxiques vous semblent-elle correcte ? Si non donnez 2 exemples de mauvais étiquetage et une hypothèse.\n",
    "\n",
    "- Votre réponse\n",
    "\n",
    "5. Grâce à ces fonctions pré-traitez tous les documents.\n",
    "6. Quel sujet est commun à chaque quinquennat étudié ? (concatener les discours de chaque quinquennat et regarder les mot communs)\n",
    "\n",
    "- Votre réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693de9b9-2263-4936-8f35-bfb7d9f663d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "# Les opérateurs de glob correspondent à l'opérateur * dans les commandes bash.\n",
    "\n",
    "def load_document(doc):\n",
    "    # utilisez ma_str.split, ma_str.join et ma_str.strip pour supprimer l'URL et le titre des documents\n",
    "    return doc\n",
    "\n",
    "data = []\n",
    "for file_name in glob('discours_voeux/*.txt'):\n",
    "    # Pour chaque fichier\n",
    "    with open(file_name) as f:\n",
    "        # On l'ouvre et on le lis\n",
    "        doc = f.read()\n",
    "        doc = load_document(doc)\n",
    "        name = os.path.basename(file_name)\n",
    "        name = name.split('.')[0]\n",
    "        quin, year, pres = name.split('-')\n",
    "        data.append((quin, year, pres, doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03423a-529d-44d9-8b0c-bd4ff8fdeec6",
   "metadata": {},
   "source": [
    "### Sauvegarde sur le disque\n",
    "\n",
    "Le choix du format de stockage de document pré-traité n'est pas trivial, nous proposons ici d'utilise le format jsonl qui permet de sauvegarder les données au format json. Cette n'est ni la meilleure ni la seules, tout dépend de l'utilisation qui sera faite des données, de la taille des fichiers, etc.\n",
    "\n",
    "Assurez vous que vous pouvez charger vos données après les avoir sauvegardé !\n",
    "\n",
    "```python\n",
    "# Sauvegarder les données\n",
    "with open('path/to/file.jsonl', 'w') as f:\n",
    "    for doc in documents:\n",
    "        # Chaque ligne devient un dictionnaire python\n",
    "        r = {\n",
    "            'year': (), 'quinq': (), 'pres': (),\n",
    "            'doc': (), # le document original non prétraité\n",
    "            'doc_pret': () # la version prétraitée du document\n",
    "        }\n",
    "        # Chaque dictionnaire est serialisé en json\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "# Charger les données\n",
    "with open('path/to/file.jsonl') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed533ed-8cb1-46f8-9357-47bdc452b6c6",
   "metadata": {},
   "source": [
    "## Exercice 2: Exploration des données\n",
    "\n",
    "En utilisant les fichiers précédemment pré-traités, extrayez et visualisez à l'aide de graphiques ou forme textuelle les informations suivantes:\n",
    "\n",
    "1. la longueur des documents en termes de caractères et de mots pour l'ensemble du corpus et par président\n",
    "   * y a-t-il une différence notable entre chaque président ?\n",
    "\n",
    "|        | Carac | Mots  |\n",
    "|--------|-------|-------|\n",
    "|Corpus  |       |       |\n",
    "|Sarkozy |       |       |\n",
    "|Hollande|       |       |\n",
    "|Macron  |       |       |\n",
    "\n",
    "\n",
    "2. la fréquence des mots et formes racinées pour l'ensemble du corpus\n",
    "    - Y a-t-il une différence les 10 premieres racines et mots ? Laquelle ?\n",
    "- Votre réponse\n",
    "\n",
    "3. Faire un graphique représentant la fréquence des mots par ordre décroissant (avec une échelle logarithmique).\n",
    "   * Vous devez observer la [loi de Zipf](https://fr.wikipedia.org/wiki/Loi_de_Zipf#Gen%C3%A8se): seuls quelques mots constituent une grande partie du corpus.\n",
    "   * Ces mots qui apparaissent souvent n'apportent généralement que peu d'information, on dit que ce sont des mots vides (stopwords), contrairement aux mots plein (en général noms, adjectifs, verbes, ...). Il est courant de les filtrer pour ne pas surcharger les modèles. Des listes de stopwords sont disponibles dans `nltk.corpus.stopwords.words`, chaque bibliothèque de TAL possède en général sa liste.\n",
    "   * /!\\\\ Il est important d'utiliser une liste compatible avec le tokeniseur utilisé. En effet, il est fréquent que le tokeniseur segmente `\"puisqu'elle\"` en `[\"puisqu'\", 'elle']` mais que la liste de mots vide contienne `puisqu'elle` mais pas `puisqu'` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbad1e-15e4-45e3-9f37-510efe5b88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7e823-08b8-47e7-8bca-8882114c20d8",
   "metadata": {},
   "source": [
    "4. Combien de mots n'apparraissent qu'une seule fois ? On appelle ces mots des hapax (hapax legomena)\n",
    "- Votre réponse\n",
    "  \n",
    "3. Afficher les 10 n-grammes (de 1 à 3) les plus fréquent (la bibliothèque `nltk` permet cela) pour l'ensemble du corpus et par président."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac813a-3e76-4e45-8274-5cebf8984284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc7b50-10c3-4a28-af15-ab320379a6db",
   "metadata": {},
   "source": [
    "4. Afficher les 10 noms, verbes, adverbes et adjectifs les plus fréquents pour l'ensemble du corpus et par président"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdef92-021b-434d-b808-e03d0d9f87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0ef5-0707-43f7-8d93-2449a980e717",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding\n",
    "\n",
    "A l'aide de la bibliothèque [`tokenizers`](https://huggingface.co/docs/tokenizers/index) et du code ci-dessous.\n",
    "\n",
    "1. Comparez la tokenisation en sous-mots du discours de Hollande en 2015 avec les modèles `'camembert/camembert-base'` et `'bert-base-uncased'`.\n",
    "    - Le modèle camembert à été entraîné sur des données en française et bert-base sur des données anglaises.\n",
    "    - Quelle différence observez-vous et formulez une hypothèse.\n",
    "  \n",
    "- Votre réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66768b16-daae-4963-9d25-2d687f243f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(MODELE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54240f79-f1cd-46c2-b86f-8df8e6a6aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(MONTEXTE).tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f07637-75cf-47a3-946e-1bc2e428a02f",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "\n",
    "Une autre bibliothèque pour l'analyse de texte est [`spacy`](https://spacy.io/). Sa philosophie est différente de nltk (qui ne travaille qu'avec des listes), avec `spacy` tout est un objet.\n",
    "\n",
    "1. Installez le modèle français pour spacy\n",
    "2. Créez une fonction `pretreat_spacy` qui retourne la même chose, mais n'utilise que spacy. Est-ce que toutes les informations sont disponibles ?\n",
    "3. Y a-t-il des différence dans l'étiquetage morphosyntaxique entre spacy et nltk ?\n",
    "    - Donnez 3 exemples s'il y en a..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031e0da-a588-4e59-9c47-45d60a08612e",
   "metadata": {},
   "source": [
    "# Analyse textuelle\n",
    "\n",
    "Avec les outils utilisés jusqu'a présent essayez de répondre aux questions suivantes:\n",
    "\n",
    "5. Comment identifier les thèmes principaux abordés par chaque président ?\n",
    "6. Y a-t-il des différence importante de vocabulaire entre Sarkozy et Macron ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595a3b8-d9e7-422f-b4eb-efdb3c939efd",
   "metadata": {},
   "source": [
    "### Paquets/commandes utiles:\n",
    "\n",
    "* Ensembles en python: `vocab = set('a b b b c'.split()))`, ainsi que les intersection `set('abc') & set('bc')`, difference `set('abc') - set('bc')`, combinaison `set('abc') | set('bc')`.\n",
    "* `collections.Counter`: un dictionnaire qui compte les occurence d'un élement\n",
    "* Mesurer le temps d'execution d'une commande dans un jupyter notebook\n",
    "\n",
    "```\n",
    "%%time  # pour une cellule entière\n",
    "code python\n",
    "\n",
    "%time code python # pour une ligne\n",
    "```\n",
    "\n",
    "* Pour faire des graphiques\n",
    "  * [matplotlib](https://matplotlib.org)\n",
    "  * [pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) offre des moyens assez simple de faire des graphiques.\n",
    "  * [seaborn](https://seaborn.pydata.org) fait de beaux graphiques.\n",
    "\n",
    "```\n",
    "# exemple minimal du graphique de la fonction x^2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(-10, 10), [i**2 for i in range(-10, 10)])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Pour simplifier les traitements utilisez des compréhensions de liste facile à lire.\n",
    "```python\n",
    "tmp_list = []\n",
    "for t in tokens:\n",
    "    t = t.replace('ü', 'u)\n",
    "    tmp_list.append(t)\n",
    "tokens = tmp_list\n",
    "\n",
    "tokens = [t.replace('ü', 'u') for t in tokens]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef775b-3d29-4fb7-ad4c-416f31849abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
